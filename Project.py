# SENG 472 - LLM Powered Software Development
# Project: Text-to-SQL Chatbot (Phase 3 - Modular Router Architecture)
# ---------------------------------------------------------------

# --- 0. Required Libraries ---
# !pip install google-generativeai pydantic pandas gradio

import google.generativeai as genai
import sqlite3
import os
import json
import gradio as gr # Importing Gradio for the UI
from pydantic import BaseModel, Field
from pathlib import Path
from typing import List, Dict, Any, Optional , Literal

# --- 1. CONFIGURATION: API Key and Database Path ---

def configure_gemini_api():
    """Loads the Gemini API key from Colab Secrets or environment variables."""
    
    # --- !! SECURITY WARNING !! ---
    # You had a hardcoded API key in your code. I've removed it to protect your key.
    # Please use Colab Secrets or environment variables as shown below.
    # DO NOT share your API key publicly.
    API_KEY = None
    try:
        # Try to get it from Colab Secrets first
        from google.colab import userdata
        API_KEY = userdata.get('GEMINI_API_KEY')
    except ImportError:
        # If not on Colab, fall back to environment variables
        API_KEY = os.getenv('GEMINI_API_KEY')

    if not API_KEY:
        raise ValueError("GEMINI_API_KEY not found. Please set it as a Colab Secret or Environment Variable.")
        
    genai.configure(api_key=API_KEY)
    print("Gemini API configured successfully.")

DB_PATH = "northwind.db"


# --- 2. JSON SCHEMAS (Pydantic Models) ---
# These are the "blueprints" we force the LLM to follow for its answers.

# --- Schema 1: User Intent Model ---
class UserIntent(BaseModel):
    """
    Pydantic model to classify the user's intent. 
    This is the "brain" of our main Router.
    """
    intent: Literal["SQL_QUERY", "GREETING", "OFF_TOPIC", "CLARIFY"] = Field(
        description="Classify the user's intent: "
                    "SQL_QUERY (a question about the database), "
                    "GREETING (a general chat), "
                    "OFF_TOPIC (unrelated), "
                    "CLARIFY (too vague)."
    )

# --- Tool Schema 1: SQL Query ---
class SQLQuery(BaseModel):
    """(Tool 1) Pydantic model to hold the SQL query generated by the model."""
    sql_query: str = Field(description="A valid SQLite SQL query to be executed on the database.")

# --- Final Output Schema ---
class FinalResponse(BaseModel):
    """
    (Final Output) The final, structured JSON response for the manager.
    'data_table' type was updated to 'Any' to prevent a 400 API error.
    """
    natural_language_answer: str = Field(
        description="A summary answer in natural language (matching the user's query language)."
    )
    data_table: Optional[Any] = Field(
        description="The data table retrieved from the database, formatted as a list of dictionaries."
    )
    sql_query_used: Optional[str] = Field(
        description="The SQL query that was used to retrieve this data."
    )


# --- 3. DATABASE HELPER FUNCTIONS ---

# Let's read the schema once and cache it globally. No need to read the file every time.
DB_SCHEMA_CACHE = None

def get_database_schema(db_path: str) -> str:
    """
    Reads the database schema. Returns from cache if already read.
    """
    global DB_SCHEMA_CACHE
    if DB_SCHEMA_CACHE:
        return DB_SCHEMA_CACHE

    if not Path(db_path).exists():
        raise FileNotFoundError(f"Database file not found: {db_path}")
        
    schema_str = ""
    try:
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            for table in tables:
                table_name = table[0]
                cursor.execute(f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table_name}';")
                create_table_sql = cursor.fetchone()[0]
                schema_str += create_table_sql + ";\n\n"
        
        DB_SCHEMA_CACHE = schema_str.strip()
        print(f"--- Database Schema ('{db_path}') read and cached successfully. ---")
        return DB_SCHEMA_CACHE
        
    except sqlite3.Error as e:
        print(f"SQLite error (get_database_schema): {e}")
        return ""

def execute_sql_query(db_path: str, query: str) -> Dict[str, Any]:
    """Executes the given SQL query and returns the results as a dictionary."""
    try:
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(query)
            results = cursor.fetchall()
            # Get column names from the cursor description
            column_names = [description[0] for description in cursor.description] if cursor.description else []
            return {"results": results, "columns": column_names, "error": None}
    except sqlite3.Error as e:
        print(f"SQLite error (execute_sql_query): {e}")
        return {"error": f"SQL Error: {e}"}


# --- 4. MODULAR LLM "TOOLS" ---
# These are the specialized LLMs that do one job really well.

def get_sql_from_natural_language(user_query: str, db_schema: str) -> Optional[str]:
    """
    (Tool 1: Text-to-SQL) Converts natural language to SQL.
    The prompt is optimized for security (SELECT-only) and SQLite dialect.
    """
    
    # --- OPTIMIZED ENGLISH SYSTEM PROMPT (Text-to-SQL) ---
    system_prompt = f"""
    You are a Text-to-SQL expert specializing in SQLite.
    YOUR SOLE TASK: Analyze the user's query and output ONLY a JSON object
    matching the 'SQLQuery' schema. NEVER provide explanations.

    SECURITY RULES:
    1.  NEVER generate commands like `INSERT`, `UPDATE`, `DELETE`, `DROP`, `ALTER`, `TRUNCATE`.
    2.  ONLY generate `SELECT` queries.
    3.  If the user asks to modify data (e.g., "Delete a customer"),
        return a JSON with `sql_query` set to "ERROR: Only read-only queries are allowed."

    PERFORMANCE & ACCURACY RULES:
    1.  Your query must be valid for the provided DATABASE SCHEMA.
    2.  Your query must be compatible with the SQLite dialect.
    3.  Join tables correctly when necessary.

    --- DATABASE SCHEMA ---
    {db_schema}
    --- DATABASE SCHEMA END ---

    Your output MUST BE ONLY the 'SQLQuery' JSON.
    """

    # Specify the model you're using (e.g., gemini-pro-latest)
    model_name_to_use = "gemini-pro-latest"
    
    model = genai.GenerativeModel(
        model_name=model_name_to_use,
        system_instruction=system_prompt,
        generation_config=genai.GenerationConfig(
            response_mime_type="application/json",
            response_schema=SQLQuery,
            temperature=0.0  # Set to 0.0 for deterministic, non-creative SQL
        )
    )
    try:
        response = model.generate_content(user_query)
        validated_output = SQLQuery.model_validate_json(response.text)
        return validated_output.sql_query
    except Exception as e:
        print(f"Gemini API error (get_sql_from_natural_language): {e}")
        return None

def get_final_json_response(original_query: str, sql_query: str, db_results: List[tuple], column_names: List[str]) -> Optional[FinalResponse]:
    """
    (Tool 2: Data-to-JSON) Analyzes raw data and creates the final JSON.
    The prompt is optimized to reply in the user's original language (TR/EN).
    
    NOTE: This function is only called if you are NOT on the free tier,
    as it's the 3rd API call in the chain.
    """
    
    # --- OPTIMIZED ENGLISH SYSTEM PROMPT (Summarizer) ---
    system_prompt = """
    You are an executive assistant.
    YOUR SOLE TASK: Analyze the provided raw database results and 
    structure them into the 'FinalResponse' JSON format.

    RULES:
    1.  LANGUAGE RULE: The 'natural_language_answer' field MUST match the
        language of the user's 'original_query' (Reply in Turkish to a Turkish query).
    2.  SUMMARY RULE: The 'natural_language_answer' should be a brief,
        clear summary of the database results.
        (e.g., "6 customers were found in London.")
    3.  DATA FORMATTING RULE: The 'data_table' field must be a JSON list
        of dictionaries, created by combining 'db_results' (rows) and 'column_names'.
    4.  SQL RULE: The 'sql_query_used' field must be identical to the 'sql_query' provided.

    Your output MUST BE ONLY the 'FinalResponse' JSON.
    """
    
    prompt = f"""
    Please analyze the following data and respond in the FinalResponse JSON format:

    User's Original Query: "{original_query}"
    Executed SQL Query: "{sql_query}"
    Database Column Names: {column_names}
    Raw Database Results (Rows): {db_results}
    """
    
    # Specify the model you're using
    model_name_to_use = "gemini-pro-latest"
    
    model = genai.GenerativeModel(
        model_name=model_name_to_use,
        system_instruction=system_prompt,
        generation_config=genai.GenerationConfig(
            response_mime_type="application/json",
            response_schema=FinalResponse,
            temperature=0.2  # A slight temperature bump for more natural-sounding summaries
        )
    )
    try:
        response = model.generate_content(prompt)
        validated_output = FinalResponse.model_validate_json(response.text)
        return validated_output
    except Exception as e:
        print(f"Gemini API error (get_final_json_response): {e}")
        return None

# --- 5. MAIN WORKFLOWS (Orchestrators) ---

def run_sql_pipeline(user_query: str, db_schema: str) -> FinalResponse:
    """
    The full pipeline that runs when an 'SQL_QUERY' intent is detected.
    (This is the function you would modify if you're on the free tier)
    """
    print(f"[SQL Pipeline] Initiated. Query: '{user_query}'")
    
    # 1. Step: Text-to-SQL (API Call 2)
    sql_query = get_sql_from_natural_language(user_query, db_schema)
    if not sql_query:
        return FinalResponse(
            natural_language_answer="Sorry, I couldn't convert your query into a valid SQL statement.",
            data_table=[],
            sql_query_used="N/A"
        )
    
    # Check for the error message we defined in the security prompt
    if "ERROR:" in sql_query:
         return FinalResponse(
            natural_language_answer=sql_query, # Pass the error to the user
            data_table=[],
            sql_query_used="N/A"
        )
        
    print(f"[SQL Pipeline] Generated SQL: {sql_query}")

    # 2. Step: SQL-Execute (No API Call)
    db_data = execute_sql_query(DB_PATH, sql_query)
    if db_data["error"]:
        return FinalResponse(
            natural_language_answer=f"An error occurred while running the SQL query: {db_data['error']}",
            data_table=[],
            sql_query_used=sql_query
        )
        
    results = db_data["results"]
    columns = db_data["columns"]
    print(f"[SQL Pipeline] Database Result: {len(results)} rows returned.")

    # 3. Step: Data-to-JSON (API Call 3 - The "Summarizer")
    # If you are on the free tier, you must replace this block
    # with the manual Python formatter from our previous conversation.
    
    if not results:
        return FinalResponse(
            natural_language_answer="No data was found matching your query.",
            data_table=[],
            sql_query_used=sql_query
        )
    
    # This is where the 3rd API call happens.
    final_response = get_final_json_response(user_query, sql_query, results, columns)
    
    if not final_response:
        return FinalResponse(
            natural_language_answer="Data was retrieved successfully, but an error occurred while interpreting the results.",
            data_table=[], # Add an empty list just in case
            sql_query_used=sql_query
        )
        
    print("[SQL Pipeline] Completed successfully.")
    return final_response

def get_user_intent(user_query: str, db_schema: str, history: List[str]) -> Optional[UserIntent]:
    """
    (Main Router - API Call 1) Classifies the user's intent.
    Prompt is now fully in English for maximum reliability and reasoning.
    It explicitly tells the model how to map Turkish words to the English schema.
    """
    
    # --- FULLY OPTIMIZED ENGLISH SYSTEM PROMPT (v4 - Router) ---
    system_prompt = f"""
    You are a highly precise database query classifier agent.
    YOUR SOLE AND ONLY TASK: Analyze the user's query and output ONLY a JSON 
    classification matching the 'UserIntent' schema. 
    NEVER answer the query directly. NEVER provide explanations.

    CLASSIFICATION PROCESS:
    1.  Read the user's new query ("NEW USER QUERY"). The user may ask in Turkish or English.
    2.  Analyze the DATABASE SCHEMA provided below (which is in English).
    3.  Decide: Can this query be answered using the tables AND columns in this schema?

    --- DATABASE SCHEMA ---
    {db_schema}
    --- DATABASE SCHEMA END ---

    CRITICAL ANALYSIS RULES (You MUST follow these):
    
    -   CRITICAL RULE 1 (Turkish 'stok' -> 'UnitsInStock'): 
        The Turkish words 'stok', 'stokta olmayan', or 'stok durumu' 
        are directly related to the 'Products.UnitsInStock' column. 
        Therefore, these queries MUST BE classified as 'SQL_QUERY'.
        
    -   CRITICAL RULE 2 (Missing 'Salary' Column): 
        The Turkish word 'maaÅŸ' or the English word 'salary' is 'OFF_TOPIC' 
        because a 'Salary' column DOES NOT EXIST in the schema (e.g., in 'Employees').
        
    -   CRITICAL RULE 3 (General):
        -   Queries about "sales", "orders", "satÄ±ÅŸ", "sipariÅŸ" -> 'SQL_QUERY'
        -   Queries about "customers", "city", "mÃ¼ÅŸteri", "ÅŸehir" -> 'SQL_QUERY'
        -   Queries unrelated to the schema (e.t., "weather", "tell me a joke") -> 'OFF_TOPIC'

    CLASSIFICATION CATEGORIES:
    1.  SQL_QUERY: The query CAN be answered using the schema (See Rule 1).
    2.  OFF_TOPIC: The query CANNOT be answered using the schema (See Rule 2).
    3.  GREETING: General greetings (merhaba, hello, thanks).
    4.  CLARIFY: The query is too vague (e.g., "Info?", "SipariÅŸler?").
    
    Your output MUST BE ONLY the 'UserIntent' JSON.
    """
    
    full_prompt_list = []
    if history:
        full_prompt_list.append("--- Conversation History ---")
        full_prompt_list.append("\n".join(history))
        full_prompt_list.append("--- End Conversation History ---")
    
    # We prefix this so the model knows which part is the new query
    full_prompt_list.append(f"NEW USER QUERY: \"{user_query}\"")
    prompt_for_api = full_prompt_list
    
    # Specify the model you're using (e.g., "gemini-pro-latest")
    model_name_to_use = "gemini-pro-latest" 
    
    model = genai.GenerativeModel(
        model_name=model_name_to_use,
        system_instruction=system_prompt,
        generation_config=genai.GenerationConfig(
            response_mime_type="application/json",
            response_schema=UserIntent,
            temperature=0.0  # Set to 0.0 for deterministic, non-creative classification
        )
    )
    
    response = None 
    try:
        response = model.generate_content(prompt_for_api) 
        validated_output = UserIntent.model_validate_json(response.text)
        return validated_output
    except Exception as e:
        # This is our little debug block, very useful
        print(f"\n!!!! ERROR (get_user_intent) !!!!")
        print(f"Error Message: {e}")
        raw_text = "Response not received (API call probably failed)."
        if response and hasattr(response, 'text'):
            raw_text = response.text
        print(f"[Debug] Raw response received: {raw_text}")
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        return None

# --- 6. GRADIO INTERFACE AND MAIN LOGIC ---

def chatbot_response(message: str, history: List[List[str]]):
    """
    This is the main function called by the Gradio ChatInterface.
    It manages the entire modular flow (Router -> Tools).
    """
    
    # 1. Configure the API and Schema
    try:
        # It's okay to call this every time for a demo, it's quick.
        configure_gemini_api() 
        db_schema = get_database_schema(DB_PATH)
        if not db_schema:
            raise Exception(f"Database schema ('{DB_PATH}') could not be read.")
    except Exception as e:
        return f"Critical Error: {e}"

    # Convert Gradio's history format (List[List[str]]) into a simple list of strings
    # that our LLM can understand.
    simple_history = []
    for user_msg, bot_msg in history:
        simple_history.append(f"User: {user_msg}")
        simple_history.append(f"Assistant: {bot_msg}")
        
    print(f"\n--- New Query Received: '{message}' ---")

    # 2. Step: Classify Intent (Router) - This is API Call 1
    intent_data = get_user_intent(message, db_schema, simple_history)
    
    if not intent_data:
        # This happens if the get_user_intent function fails
        return "Sorry, an error occurred while understanding your intent. Check the console."
        
    intent = intent_data.intent
    print(f"[Router] Detected Intent: {intent}")

    # 3. Step: Decide based on intent (Orchestrator Logic)
    
    final_response = None
    
    if intent == "SQL_QUERY":
        # If it's a SQL query, run the full pipeline
        final_response_model = run_sql_pipeline(message, db_schema)
        final_response = final_response_model.model_dump()
        
    elif intent == "GREETING":
        # Just say hello and guide the user
        final_response_model = FinalResponse(
            natural_language_answer="Hello! I am your Northwind database assistant. "
                                    "How can I help you with orders, customers, or products? "
                                    "\n\nFor example, you can ask: 'Who are my customers in London?'",
            data_table=[],
            sql_query_used="N/A"
        )
        final_response = final_response_model.model_dump()

    elif intent == "OFF_TOPIC":
        # Politely decline and guide the user back
        final_response_model = FinalResponse(
            natural_language_answer="Sorry, my expertise is limited to the Northwind database. "
                                    "Please ask a question related to the database.",
            data_table=[],
            sql_query_used="N/A"
        )
        final_response = final_response_model.model_dump()
        
    elif intent == "CLARIFY":
        # Ask for more details
        final_response_model = FinalResponse(
            natural_language_answer="I couldn't quite understand your request. "
                                    "Could you please ask a more specific question?",
            data_table=[],
            sql_query_used="N/A"
        )
        final_response = final_response_model.model_dump()

    # 4. Step: Send the formatted output back to Gradio
    # The project doc wants "structured JSON output", so we'll make it look good.
    
    # Let's format the response as Markdown
    response_markdown = f"""
    ### ðŸ’¬ Natural Language Answer
    {final_response['natural_language_answer']}
    
    ---
    
    ### ðŸ“Š Data Table
    """
    
    # If data_table is not empty, let's add it
    if final_response.get('data_table'):
        # Converting the list of dicts to a Markdown table is messy.
        # Showing it as JSON is safer, faster, and fits the "structured output" brief.
        response_markdown += f"\n```json\n{json.dumps(final_response['data_table'], indent=2, ensure_ascii=False)}\n```\n"
    else:
        response_markdown += "*Data table is empty.*\n"
        
    # Let's also add the SQL query that was used, for transparency
    response_markdown += f"\n---\n*SQL Query Executed: `{final_response['sql_query_used']}`*"

    return response_markdown

# --- 7. LAUNCH THE APP ---

if __name__ == "__main__":
    print("Launching Gradio interface...")
    
    # Create the Gradio interface
    demo = gr.ChatInterface(
        fn=chatbot_response, # This function will be called on every message
        title="SENG 472 - Northwind DB Chatbot (v4 - English Prompts)",
        description="Ask questions about the Northwind database in natural language (TR/EN).",
        examples=[
            "Merhaba",
            "KaÃ§ adet mÃ¼ÅŸterim var?",
            "En Ã§ok satÄ±ÅŸ yapan 3 Ã§alÄ±ÅŸan kim?",
            "What are the names and phone numbers of my customers who live in London?",
            "Stokta kalmayan Ã¼rÃ¼nler hangileri?", # This is our key test case
            "Bana bir fÄ±kra anlat",
            "Ã‡alÄ±ÅŸanlarÄ±n maaÅŸlarÄ± ne kadar?" # And this is the other key test case
        ],
        chatbot=gr.Chatbot(height=500),
        textbox=gr.Textbox(placeholder="Please type your question here...", container=False, scale=7),
        theme="soft"
    )
    
    # Launch the interface (provides a public link if on Colab)
    demo.launch(debug=True) # debug=True is helpful for troubleshooting